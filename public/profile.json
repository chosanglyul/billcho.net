{
  "name": "SangLyul Cho",
  "email": "chosanglyul@gmail.com",
  "about": "Co-designing efficient deep learning algorithm and software.",
  "abstract": "My primary interest lies in machine learning (ML) and the acceleration of large-scale deep learning (DL) models. I have a diverse background in ML, systems, competitive programming, and web development. Having observed the escalating growth and complexity of DL models, I've developed an interest in tackling the challenges of efficient training and serving these models. To deepen my understanding and explore these interests further, I am double majoring in math.",
  "educations": [
    {
      "degree": "High School Diploma",
      "gpa": "TBD",
      "school": "Seoul Science High School",
      "duration": "Mar 2019 - Feb 2022",
      "thesis": "Senior project: Improvement of backpropagation time and stability of RNN using Tree-RNN"
    },
    {
      "degree": "B.S. Student in CSE and Math",
      "gpa": "TBD",
      "school": "Seoul National University",
      "duration": "Mar 2022 - Present"
    }
  ],
  "publications": [
    {
      "title": "VGA: Hardware Accelerator for Scalable Long Sequence Model Inference",
      "authors": ["SeungYul Lee", "Jihoon Hong", "Hyunseung Lee", "SangLyul Cho", "Jae W. Lee"],
      "journal": "The 57th IEEE/ACM International Symposium on Microarchitecture (MICRO)",
      "year": 2024,
      "links": [
        {
          "type": "Paper",
          "href": "https://ieeexplore.ieee.org/document/10764661"
        }
      ],
      "abstract": "Effectively modeling relationships between distant elements in a long input sequence is an important task that remains challenging to this day. The state-of-the-art models for processing sequential data are self-attention-based transformer models. However, the computational complexity of self-attention is quadratic to the input sequence length, which often becomes the limiting factor in scaling the sequence length. Recently, state space model (SSM)-based global convolution models, which replace attention with convolution, have been found to be effective for modeling long sequences, with a sub-quadratic complexity using Fast Fourier Transform (FFT). However, they show sub-optimal performance on data-parallel accelerators like GPU, due to the regions of extremely low compute utilization with memory bandwidth-bound operations. To address this inefficiency, this paper proposes the Vandermonde matrix Generating Accelerator (VGA), a custom accelerator that performs FFT-based convolution in an area/power-efficient manner. VGA introduces Complex number Compute Units (CCUs) to fully utilize the high on-chip SRAM bandwidth, and parameters are generated on the fly to drastically reduce the required SRAM capacity. VGA achieves 76x(48x) higher area (power) efficiency than NVIDIA A100 GPU when executing the global convolution operator of H3, a state-of-the-art SSM-based model.",
      "works": [
        "Implemented and profiled LLMs in NVIDIA GPU and Google Cloud TPU"
      ]
    },
    {
      "title": "Any-Precision LLM: Low-Cost Deployment of Multiple, Different-Sized LLMs",
      "authors": ["Yeonhong Park", "Jake Hyun", "SangLyul Cho", "Bonggeun Sim", "Jae W. Lee"],
      "journal": "The 41st International Conference on Machine Learning (ICML)",
      "year": 2024,
      "highlight": "Oral Presentation (Accept Rate: 144/9473=1.5%)",
      "links": [
        {
          "type": "Paper",
          "href": "https://openreview.net/pdf?id=u09gadH3BU"
        },
        {
          "type": "GitHub",
          "href": "https://github.com/SNU-ARC/any-precision-llm"
        },
        {
          "type": "Video",
          "href": "https://icml.cc/virtual/2024/oral/35450"
        }
      ],
      "abstract": "Recently, considerable efforts have been directed towards compressing Large Language Models (LLMs), which showcase groundbreaking capabilities across diverse applications but entail significant deployment costs due to their large sizes. Meanwhile, much less attention has been given to mitigating the costs associated with deploying multiple LLMs of varying sizes despite its practical significance. Thus, this paper introduces any-precision LLM, extending the concept of any-precision DNN to LLMs. Addressing challenges in any-precision LLM, we propose a lightweight method for any-precision quantization of LLMs, leveraging a post-training quantization framework, and develop a specialized software engine for its efficient serving. As a result, our solution significantly reduces the high costs of deploying multiple, different-sized LLMs by overlaying LLMs quantized to varying bit-widths, such as 3, 4, ..., n bits, into a memory footprint comparable to a single n-bit LLM. All the supported LLMs with varying bit-widths demonstrate state-of-the-art model quality and inference throughput, proving itself to be a compelling option for deployment of multiple, different-sized LLMs. The code is available at https://github.com/SNU-ARC/any-precision-llm.",
      "figures": [
        "/publication/any-prec-llm-iu.png",
        "/publication/any-prec-llm-sw-engine.png"
      ],
      "works": [
        "Designed and implemented the Table Lookup Merge (TLM) technique to enhance the efficiency of the quantized GEMV, achieving up to 19% faster GEMV time through optimization",
        "Developed an efficient CUDA kernel with tiling for batched and quantized GEMV"
      ]
    }
  ],
  "works": [
    {
      "title": "Samsung Electronics",
      "position": "Summer Software Intern, SW Development Team in Memory Business",
      "period": "07/2022 - 08/2022",
      "works": [
        "Improved performance of SSD(Solid State Drive) using machine learning."
      ]
    },
    {
      "title": "ARC Lab",
      "link": "https://arc.snu.ac.kr",
      "position": "Undergraduate Research Intern (Advisor: Prof. Jae W. Lee)",
      "period": "07/2023 - 08/2023, 01/2024 - 02/2024",
      "works": [
        "Published two papers under the supervision of Prof. Jae W. Lee."
      ]
    }
  ],
  "awards": [
    {
      "name": "Semiconductor Track Scholarship",
      "place": "4000+ USD",
      "period": "2024 - Present"
    },
    {
      "name": "Presidential Science Scholarship",
      "place": "30000+ USD",
      "period": "2022 - Present"
    },
    {
      "name": "ICPC Asia Seoul Regional 2023",
      "place": "Fifth Award, 12th place"
    },
    {
      "name": "Samsung Collegiate Programming Cup 2023",
      "link": "https://research.samsung.com/scpc",
      "place": "Fifth Award"
    },
    {
      "name": "Undergraduate Deep Learning Product Recognition Contest 2023",
      "link": "https://www.disu.or.kr/community/notice?md=v&bbsidx=7320/",
      "place": "Excellence Prize, 2nd place"
    },
    {
      "name": "Korea Olympiad in Informatics 2021",
      "place": "Silver Prize, 8th place"
    }
  ],
  "languages": [
    {
      "language": "Korean",
      "proficiency": "Native"
    },
    {
      "language": "English",
      "proficiency": "Advanced (TOEFL xxx)"
    }
  ]
}